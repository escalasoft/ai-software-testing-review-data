# ai-software-testing-review-data

This repository contains the supplementary materials associated with the systematic literature review on **Artificial Intelligence in Software Testing**.  
It includes all the datasets, coding artifacts, and methodological checklists required for full transparency and reproducibility in accordance with **PRISMA 2020**.

## 📁 Repository Structure

ai-software-testing-review-data/
┣ data/
┃ ┣ coding_book_taxonomy.xlsx
┃ ┗ raw_data_extraction.xlsx
┣ screening/
┃ ┗ filtering_articles_marked.xlsx
┣ tables/
┃ ┗ supplementary_tables.docx
┣ checklist/
┃ ┗ PRISMA_2020_Checklist_AEV.docx
┣ LICENSE_CC_BY_4.0.txt
┗ README.md

## 📂 File Descriptions  

### 🔹 `screening/filtering_articles_marked.xlsx`
Records the **screening and selection process**, including:
- (1) Title (2) Abstract & Keywords (3) Introduction / Conclusion (4) Full-text review  
- Extra filters: (5d) Duplicated, (6r) Retracted, (NRP) Not responding, (0) Selected → 66 studies  

---

### 🔹 `data/raw_data_extraction.xlsx`
Acts as the **raw data extraction sheet**, listing for each study:  
- Problem code (e.g., SDP, TCM, ATE)  
- Dataset name & source  
- Instances / variables  
- Algorithms used  
- Evaluation metrics (Accuracy, Precision, Recall, F1-score, ROC-AUC, MCC, etc.)  

Supports the quantitative synthesis in **Figures 7–9** and **Tables 6–7**.

---

### 🔹 `data/coding_book_taxonomy.xlsx`
Operational **coding guide** Contains three sheets:  
  1. *Algorithm_Taxonomy* (definitions and rules for algorithm categories)  
  2. *Variable_Taxonomy* (definitions for input variable categories)  
  3. *Metrics_Taxonomy* (definitions for evaluation metric categories: CP, AC, CE, AR, STS, CGD).

---

### 🔹 `tables/supplementary_tables.docx`
Extended annexes from the manuscript:  
- **Annex B:** Algorithms per study  
- **Annex C:** Variables and input features  
- **Annex D:** Evaluation metrics with formulas and definitions  

---

### 🔹 `checklist/PRISMA_2020_Checklist_AEV.docx`
Complete PRISMA 2020 checklist used to verify reporting quality and reproducibility.

---

## 🧠 Methodological Framework  

Aligned with:
- 📘 **PRISMA 2020 Statement** for transparent reporting  
- 🧩 **Kitchenham & Charters (2007)** for evidence-based SE reviews  
- 🌐 **FAIR Data Principles** (Open Science)  

Ensures:  
✅ Transparent search and selection  
✅ Replicable data extraction and coding  
✅ Independent verification of results  

---

## 🔗 Access & Citation  

**Repository:** [https://github.com/escalasoft/ai-software-testing-review-data](https://github.com/escalasoft/ai-software-testing-review-data)

> Escalante-Viteri, A.; Mauricio, D.  
> *Artificial Intelligence in Software Testing: A Systematic Review of a Decade of Evolution and Taxonomy.*  
> **Algorithms (MDPI), 2025.**

---

## ⚖️ License  

Distributed under the **Creative Commons Attribution 4.0 International (CC BY 4.0)** license.  
You may share and adapt this material with proper attribution.  
📜 [View License →](https://creativecommons.org/licenses/by/4.0/)

---

## 📬 Contact  

**Corresponding Author:** Alex Escalante-Viteri  
Universidad Nacional Mayor de San Marcos (UNMSM)  
Faculty of Systems and Informatics Engineering  
📧 [alex.escalante@unmsm.edu.pe](mailto:alex.escalante@unmsm.edu.pe)





