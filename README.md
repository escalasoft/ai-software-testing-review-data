# ai-software-testing-review-data

This repository contains the supplementary materials associated with the systematic literature review on **Artificial Intelligence in Software Testing**.  
It includes all the datasets, coding artifacts, and methodological checklists required for full transparency and reproducibility in accordance with **PRISMA 2020**.

## ğŸ“ Repository Structure

ai-software-testing-review-data/
â”£ data/
â”ƒ â”£ coding_book_taxonomy.xlsx
â”ƒ â”— raw_data_extraction.xlsx
â”£ screening/
â”ƒ â”— filtering_articles_marked.xlsx
â”£ tables/
â”ƒ â”— supplementary_tables.docx
â”£ checklist/
â”ƒ â”— PRISMA_2020_Checklist_AEV.docx
â”£ LICENSE_CC_BY_4.0.txt
â”— README.md

## ğŸ“‚ File Descriptions  

### ğŸ”¹ `screening/filtering_articles_marked.xlsx`
Records the **screening and selection process**, including:
- (1) Titleâ€ƒ(2) Abstract & Keywordsâ€ƒ(3) Introduction / Conclusionâ€ƒ(4) Full-text review  
- Extra filters: (5d) Duplicated, (6r) Retracted, (NRP) Not responding, (0) Selected â†’ 66 studies  

---

### ğŸ”¹ `data/raw_data_extraction.xlsx`
Acts as the **raw data extraction sheet**, listing for each study:  
- Problem code (e.g., SDP, TCM, ATE)  
- Dataset name & source  
- Instances / variables  
- Algorithms used  
- Evaluation metrics (Accuracy, Precision, Recall, F1-score, ROC-AUC, MCC, etc.)  

Supports the quantitative synthesis in **Figures 7â€“9** and **Tables 6â€“7**.

---

### ğŸ”¹ `data/coding_book_taxonomy.xlsx`
Operational **coding guide** Contains three sheets:  
  1. *Algorithm_Taxonomy* (definitions and rules for algorithm categories)  
  2. *Variable_Taxonomy* (definitions for input variable categories)  
  3. *Metrics_Taxonomy* (definitions for evaluation metric categories: CP, AC, CE, AR, STS, CGD).

---

### ğŸ”¹ `tables/supplementary_tables.docx`
Extended annexes from the manuscript:  
- **Annex B:** Algorithms per study  
- **Annex C:** Variables and input features  
- **Annex D:** Evaluation metrics with formulas and definitions  

---

### ğŸ”¹ `checklist/PRISMA_2020_Checklist_AEV.docx`
Complete PRISMA 2020 checklist used to verify reporting quality and reproducibility.

---

## ğŸ§  Methodological Framework  

Aligned with:
- ğŸ“˜ **PRISMA 2020 Statement** for transparent reporting  
- ğŸ§© **Kitchenham & Charters (2007)** for evidence-based SE reviews  
- ğŸŒ **FAIR Data Principles** (Open Science)  

Ensures:  
âœ… Transparent search and selection  
âœ… Replicable data extraction and coding  
âœ… Independent verification of results  

---

## ğŸ”— Access & Citation  

**Repository:** [https://github.com/escalasoft/ai-software-testing-review-data](https://github.com/escalasoft/ai-software-testing-review-data)

> Escalante-Viteri, A.; Mauricio, D.  
> *Artificial Intelligence in Software Testing: A Systematic Review of a Decade of Evolution and Taxonomy.*  
> **Algorithms (MDPI), 2025.**

---

## âš–ï¸ License  

Distributed under the **Creative Commons Attribution 4.0 International (CC BY 4.0)** license.  
You may share and adapt this material with proper attribution.  
ğŸ“œ [View License â†’](https://creativecommons.org/licenses/by/4.0/)

---

## ğŸ“¬ Contact  

**Corresponding Author:** Alex Escalante-Viteri  
Universidad Nacional Mayor de San Marcos (UNMSM)  
Faculty of Systems and Informatics Engineering  
ğŸ“§ [alex.escalante@unmsm.edu.pe](mailto:alex.escalante@unmsm.edu.pe)





